{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "from sklearn.metrics import r2_score\n",
        "from tensorflow.keras.layers import Dropout\n",
        "import pandas as pd\n",
        "import random\n",
        "from sklearn.model_selection import train_test_split\n",
        "from tensorflow.keras.optimizers import RMSprop\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.metrics import confusion_matrix\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
        "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
        "import seaborn as sns\n",
        "from sklearn.impute import SimpleImputer\n",
        "from sklearn.preprocessing import MinMaxScaler, StandardScaler\n",
        "from sklearn.impute import KNNImputer\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "from sklearn.preprocessing import PolynomialFeatures\n",
        "from sklearn.linear_model import LinearRegression\n",
        "from sklearn.pipeline import make_pipeline\n",
        "import xgboost as xgb\n",
        "from sklearn.pipeline import make_pipeline\n",
        "from sklearn.preprocessing import PolynomialFeatures\n",
        "from sklearn.linear_model import Ridge\n",
        "from sklearn.metrics import mean_squared_error\n",
        "import matplotlib.pyplot as plt"
      ],
      "metadata": {
        "id": "gBYGofQkyE_c"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Overall Notes/Ideas\n",
        "\n",
        "* Consider comparing lithium against trace elements and lithium againt major elements\n",
        "  * In other words, lithium against 'ppm' elements and lithium against 'WT%' elements."
      ],
      "metadata": {
        "id": "7sDH5LHYTFrs"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "####################################################\n",
        "\n",
        "\n",
        "The initial dataset is taken from the GEOROC geochemical database and is a large dataset of micas taken from different papers.\n",
        "\n",
        "* Column R gives the kind of mica (mineral name) – you should **exclude** celadonite, glauconite, hydromica, hydromuscovite, margarite, sericite and yangzhumingite in the first instance.\n",
        "\n",
        "* That leaves two broad kinds of mica – muscovite (which is essentially free of Fe and Mg) and biotite (which contains Fe and Mg).\n",
        "\n",
        "I’m sure you’ll have lots of questions about all this! So just get in touch when you want to.\n",
        "\n",
        "\n",
        "\n",
        "It's also worth saying that I’m sure this is also **not complete**. For example, I know there was a very recent paper **(Breiter et al. 2023)** which has a lot of data presented in figures but not in dataset form – there are probably also others. A detailed literature search might produce more, and there may be good ways to extract data from digital figures."
      ],
      "metadata": {
        "id": "wBYuLOHFw9Sr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "ini = pd.read_excel('FULL_GEOROC_2022_12_SGFTFN_MICA.xlsx')"
      ],
      "metadata": {
        "id": "y5hiXXAsyDae"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Further Instructions\n",
        "\n",
        "You can delete age information, that’s no problem.\n",
        "Age represents a geological age (i.e. timing of when the mineral formed) so is not really relevant.\n",
        "\n",
        "Similarly, you’ll find columns with **isotopic compositions (e.g. AR36(CCMSTP/G), AR39(MOL/G))** which are also not relevant to what you’re doing. Those can be ignored, hidden or deleted."
      ],
      "metadata": {
        "id": "kj4cKXxjjuyu"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Drop Non-Relevant Columns\n",
        "\n",
        "Drop He3 (CR), He4 (CS), Ne20 (CY), the Ar36-Ar40 columns (DG-DO), K40 (DQ), Ca42 (DS), Ca43 (DT), Ge73 (EG), Ge74 (EH), KR84 (EL), Mo95-100 (ES-EW), XE132 (FC), RA226 (GC) and everything from GG to JC inclusive."
      ],
      "metadata": {
        "id": "CoPTzS0EjzoS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# We can update this as we go\n",
        "non_relevant = ['AGE(KA)', 'AGE(MA)', 'CITATION', 'AR36(CCMSTP/G)',\n",
        "'AR39(CCMSTP/G)', 'AR39(MOL/G)', 'AR40(AT/G)',\n",
        "'AR40(CCMSTP/G)', 'AR40(MOL/G)', 'AR40(PPM)', 'AR40(PPB)',\n",
        "'HE3(CCMSTP/G)', 'HE4(CCMSTP/G)', 'K40(PPM)',\n",
        "'NE20(CCMSTP/G)', 'CA42(PPM)', 'CA43(PPM)', 'GE73(PPM)',\n",
        "'GE74(PPM)', 'KR84(CCMSTP/G)', 'MO95(PPM)', 'MO97(PPM)',\n",
        "'MO98(PPM)', 'MO100(PPM)', 'XE132(CCMSTP/G)', 'RA226_ACT(BQ/KG)',\n",
        "'ND143_ND144','EPSILON_ND','EPSILON_ND_INI','SM147_ND144',\n",
        "'SR87_SR86', 'RB87_SR86', 'PB206_PB204','PB207_PB204',\n",
        "'PB207_PB206', 'PB208_PB204', 'PB208_PB206','HF176_HF177', 'LU176_HF177',\n",
        "'HE4_HE3', 'K40_AR36','K40_CA44', 'AR36_AR38', 'AR36_AR39',\n",
        "'AR36_AR40','AR37_AR39', 'AR37_AR40','AR38_AR36', 'AR38_AR39', 'AR38_AR40',\n",
        "'AR39_AR36','AR39_AR40', 'AR40_AR36', 'AR40_AR38', 'AR40_AR39', 'AR40_K40', 'PB207_U235',\n",
        "'PB210_TH230_ACT', 'RA226_TH230_ACT', 'TH230_TH232_ACT', 'TH230_U238_ACT', 'TH232_PB204',\n",
        "'TH232_PB208', 'TH232_U238', 'U234_U238_ACT','U235_PB204', 'U238_PB204', 'U238_PB206',\n",
        "'U238_TH230_ACT', 'U238_TH232_ACT', 'N2(MOL/G)', 'D7LI(VS LSVEC)', 'D7LI(VS NBS8545)',\n",
        "'D7LI(VS IRMM-016)', 'D11B(VS NBS951)', 'B11_B10', 'D13C(VS VPDB)', 'D15N(PER MIL)',\n",
        "'D18O(PER MIL)', 'D18O(VS SMOW)', 'D18O(VS VSMOW)', 'D25MG(VS DSM3)', 'D26MG(VS DSM3)',\n",
        "'CA40_CA44', 'D41K(VS SRM3141A)', 'D44_40CA(VS SRM915A)', 'D44_42CA(VS SRM915A)',\n",
        "'D49TI(VS OL-TI)', 'D56FE(VS IRMM-014)', 'D57FE(VS IRMM-014)', 'D60NI(VS NBS986)',\n",
        "'D66ZN(VS JMC3-0749)', 'D68ZN(VS JMC3-0749)', 'D98_95MO(PER MIL)', 'D98MO(VS NIST3134)',\n",
        "'D98_95MO(VS NIST3134)', 'D137_134BA(VS SRM3104A)', 'D138_134BA(VS SRM3104A)',\n",
        "'DD(PER MIL)', 'DD(VS SMOW)', 'DD(VS VSMOW)', 'SM2O3(WT%)', 'EU2O3(WT%)',\n",
        "'GD2O3(WT%)','DY2O3(WT%)','SO2(WT%)','SO3(WT%)','S(WT%)','H2S(WT%)','LOI(WT%)',\n",
        "'O(WT%)', 'YB2O3(WT%)', 'PR2O3(WT%)', 'AS2O3(WT%)', 'CH4(WT%)','O2(WT%)',\n",
        "'CO(WT%)', 'H2O(WT%)', 'H2OP(WT%)', 'H2OM(WT%)','CO2(WT%)', 'OH(WT%)',\n",
        "'HE3(CCMSTP/G)', 'HE4(CCMSTP/G)', 'NE20(CCMSTP/G)', 'AR36(CCMSTP/G)',\n",
        "'AR39(CCMSTP/G)', 'AR39(MOL/G)', 'AR40(AT/G)', 'AR40(CCMSTP/G)', 'AR40(MOL/G)',\n",
        "'AR40(PPM)', 'AR40(PPB)', 'K40(PPM)', 'CA42(PPM)', 'CA43(PPM)', 'GE73(PPM)',\n",
        "'GE74(PPM)', 'KR84(CCMSTP/G)', 'MO95(PPM)', 'MO97(PPM)', 'MO98(PPM)',\n",
        "'MO100(PPM)', 'XE132(CCMSTP/G)', 'ALBITE(MOL%)', 'ANORTHITE(MOL%)', 'ANNITE(MOL%)',\n",
        "'ENSTATITE(MOL%)', 'FERROSILITE(MOL%)', 'MARGARITE(MOL%)','MUSCOVITE(MOL%)',\n",
        "'ORTHOCLASE(MOL%)', 'PARAGONITE(MOL%)','PHLOGOPITE(MOL%)', 'WOLLASTONITE(MOL%)',\n",
        "'N(PPM)', 'S(PPM)', 'BR(PPM)', 'GRAIN SIZE', 'SAMPLE NAME']\n",
        "\n",
        "# You can scrap AJ-AN inclusive, AU, BR, CB-CI inclusive and CP.\n",
        "# Other elements that won’t be of interest for your regression include BS-BV, CA"
      ],
      "metadata": {
        "id": "_hNknQCGjzaO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Drop Non-Relevant Columns\n",
        "df = ini.drop(non_relevant, axis = 1)\n",
        "df.columns"
      ],
      "metadata": {
        "id": "QLlRPaBu0HyD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Exclude** CELADONITE, GLAUCONITE, HYDROMICA, HYDROMUSCOVITE, MARGARITE, SERICITE and YANGZHUMINGITE from the column titled 'MINERAL'"
      ],
      "metadata": {
        "id": "sKqUx_GZ2I_F"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df['MINERAL'].value_counts()"
      ],
      "metadata": {
        "id": "HslPeFtt2UUm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "excluded = ['CELADONITE', 'GLAUCONITE', 'HYDROMICA', 'HYDROMUSCOVITE',\n",
        "            'MARGARITE', 'SERICITE', 'YANGZHUMINGITE']"
      ],
      "metadata": {
        "id": "hEqnw6Td3KLb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "extract = ~df['MINERAL'].isin(excluded)\n",
        "df = df[extract]"
      ],
      "metadata": {
        "id": "i-KMSA0X5c8-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df['MINERAL'].value_counts()"
      ],
      "metadata": {
        "id": "Cv_gGF-p6V98"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Mol Calculations"
      ],
      "metadata": {
        "id": "ax1b8W_Q_nlL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# We will write mol, wt%, and ppm calculation functions here\n",
        "\n",
        "def wt2ppm(item): # to ppm\n",
        "  return item * 10000\n",
        "\n",
        "def ppm2wt(item): # to WT%\n",
        "  return item / 10000\n",
        "\n",
        "def ppb2m(item): # to ppm\n",
        "  return item / 1000\n",
        "\n",
        "def ppb2wt(item): # to WT%\n",
        "  return item / 10000000\n"
      ],
      "metadata": {
        "id": "8YzFcx0jz3B4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Functions to convert materials from their oxide form to elements\n",
        "def ZRO2(item):\n",
        "  return item*(91.2/(91.2 + 32))\n",
        "\n",
        "def HFO2(item):\n",
        "  return item*(178.5/(178.5 + 32))\n",
        "\n",
        "def THO2(item):\n",
        "  return item*(232/(232 + 32))\n",
        "\n",
        "def UO2(item):\n",
        "  return item*(238/(238 + 32))\n",
        "\n",
        "def UO3(item):\n",
        "  return item*(238/(238 + 48))\n",
        "\n",
        "def CR2O3(item):\n",
        "  return item*((2*52)/(2*52 + 48))\n",
        "\n",
        "def LA2O3(item):\n",
        "  return item*((2*138.9)/(2*138.9 + 48))\n",
        "\n",
        "def CE2O3(item):\n",
        "  return item*((2*140.1)/(2*140.1 + 48))\n",
        "\n",
        "def ND2O3(item):\n",
        "  return item*((2*144.2)/(2*144.2 + 48))\n",
        "\n",
        "def Y2O3(item):\n",
        "  return item*((2*88.9)/(2*88.9 + 48))\n",
        "\n",
        "def V2O3(item):\n",
        "  return item*((2*50.9)/(2*50.9 + 48))\n",
        "\n",
        "def V2O5(item):\n",
        "  return item*((2*50.9)/(2*50.9 + 80))\n",
        "\n",
        "def NB2O3(item):\n",
        "  return item*((2*92.9)/(2*92.9 + 48))\n",
        "\n",
        "def NB2O5(item):\n",
        "  return item*((2*92.9)/(2*92.9 + 80))\n",
        "\n",
        "def TA2O5(item):\n",
        "  return item*((2*180.9)/(2*180.9 + 80))\n",
        "\n",
        "def WO3(item):\n",
        "  return item*((183.8)/(183.8 + 48))\n",
        "\n",
        "def BAO(item):\n",
        "  return item*(137.3/(137.3 + 16))\n",
        "\n",
        "def SRO(item):\n",
        "  return item*(87.6/(87.6 + 16))\n",
        "\n",
        "def PBO(item):\n",
        "  return item*(207.2/(207.2 + 16))\n",
        "\n",
        "def SNO2(item):\n",
        "  return item*(118.7/(118.7 + 32))\n",
        "\n",
        "def NIO(item):\n",
        "  return item*(58.7/(58.7 + 16))\n",
        "\n",
        "def ZNO(item):\n",
        "  return item*(65.4/(65.4 + 16))\n",
        "\n",
        "def COO(item):\n",
        "  return item*(58.9/(58.9 + 16))\n",
        "\n",
        "def CUO(item):\n",
        "  return item*(63.5/(63.5 + 16))\n",
        "\n",
        "def CS2O(item):\n",
        "  return item*((2*132.9)/(2*132.9 + 16))\n",
        "\n",
        "def RB2O(item):\n",
        "  return item*((2*85.5)/(2*85.5 + 16))\n",
        "\n",
        "def LI2O(item):\n",
        "  return item*((2*6.9)/(2*6.9 + 16))\n",
        "\n",
        "def F2O(item):\n",
        "  return item*((2*19)/(2*19 + 16))\n",
        "\n",
        "def CL2O(item): # BZ\n",
        "  return item*((2*35.5)/(2*35.5 + 16))\n",
        "\n",
        "\n",
        "##############################################################################\n",
        "\n",
        "# You can scrap AJ-AN inclusive, AU, BR, CA-CI inclusive and CP.\n",
        "# Other elements that won’t be of interest for your regression include BS-BV"
      ],
      "metadata": {
        "id": "3Dm1r844OsQ0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Functions to convert certain elements to their oxide form\n",
        "# Elements that should be left as oxide include SiO2, TiO2,\n",
        "# Al2O3, FeO (or Fe2O3), MnO, MgO, CaO, Na2O, K2O, P2O5, H2O (if present).\n",
        "\n",
        "# SiO2\n",
        "def Si(revert):\n",
        "  return revert*((28.1 + 32)/28.1)\n",
        "\n",
        "# Al2O3\n",
        "def Al(revert):\n",
        "  return revert*((2*27 + 48)/(2*27))\n",
        "\n",
        "# Fe2O3 to FeO\n",
        "def FeO(revert):\n",
        "  return revert*((2*(55.8 + 16))/(2*55.8 + 48))\n",
        "\n",
        "# Fe to FeO\n",
        "def Fe(revert):\n",
        "  return revert*((55.8 + 16)/(58.8))\n",
        "\n",
        "# MgO\n",
        "def Mg(revert):\n",
        "  return revert*((24.3 + 16)/24.3)\n",
        "\n",
        "# Na2O\n",
        "def Na(revert):\n",
        "  return revert*((2*23 + 16)/(2*23))\n",
        "\n",
        "# K2O\n",
        "def K(revert):\n",
        "  return revert*((2*39.1 + 16)/(2*39.1))\n",
        "\n",
        "# P2O5\n",
        "def P(revert):\n",
        "  return revert*((2*31 + 80)/(2*31))\n",
        "\n",
        "# CaO\n",
        "def Ca(revert):\n",
        "  return revert*((40.1 + 16)/40.1)\n",
        "\n",
        "# MnO\n",
        "def Mn(revert):\n",
        "  return revert*((54.9 + 16)/54.9)"
      ],
      "metadata": {
        "id": "ktD6NF7POvYV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def convert_mixed_value(value):\n",
        "    if isinstance(value, (float, int)):\n",
        "        return float(value)\n",
        "    if \"\\\\\" in value:\n",
        "        parts = value.split('\\\\')\n",
        "        number = parts[0].strip()\n",
        "        return float(number)\n",
        "    else:\n",
        "        return float(value)"
      ],
      "metadata": {
        "id": "RJzIyWSGqWaK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Conversions and Merging\n",
        "\n",
        "#### Notes\n",
        "\n",
        "* FeOT and FeO should be merged in advance\n",
        "* Same with Fe2O3T and Fe2O3"
      ],
      "metadata": {
        "id": "mRyc9zu-d7c9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Pre-Merging"
      ],
      "metadata": {
        "id": "nWAF56TMi704"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Merge FeOT and FeO\n",
        "df['FEO(WT%)'] = df['FEOT(WT%)'].fillna(df['FEO(WT%)'])\n",
        "# Merge Fe2O3T and Fe2O3\n",
        "df['FE2O3(WT%)'] = df['FE2O3T(WT%)'].fillna(df['FE2O3(WT%)'])"
      ],
      "metadata": {
        "id": "iVJFriIfcJYS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Transforming Columns to Floats\n",
        "\n",
        "Some columns contain non float and non int type data. This makes them incompatible with our above functions.\n",
        "\n",
        "We will iterate over these values to reduce code duplication and ensure conversion consistency.\n",
        "\n"
      ],
      "metadata": {
        "id": "CxbapViui_m3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "_numericals = ['SIO2(WT%)', 'TIO2(WT%)', 'AL2O3(WT%)', 'FEO(WT%)', 'CAO(WT%)',\n",
        "    'MGO(WT%)', 'MNO(WT%)', 'K2O(WT%)', 'NA2O(WT%)', 'P2O5(WT%)', 'LI(PPM)',\n",
        "    'BE(PPM)', 'B(PPM)', 'F(PPM)', 'CL(PPM)', 'SC(PPM)',\n",
        "    'TI(PPM)', 'V(PPM)', 'CR(PPM)', 'CO(PPM)', 'NI(PPM)', 'CU(PPM)',\n",
        "    'ZN(PPM)', 'GA(PPM)', 'GE(PPM)', 'AS(PPM)', 'SE(PPM)',\n",
        "    'RB(PPM)', 'SR(PPM)', 'Y(PPM)', 'ZR(PPM)', 'NB(PPM)', 'MO(PPM)',\n",
        "    'AG(PPM)', 'CD(PPM)', 'IN(PPM)', 'SN(PPM)', 'SB(PPM)', 'TE(PPM)',\n",
        "    'CS(PPM)', 'BA(PPM)', 'LA(PPM)', 'CE(PPM)', 'PR(PPM)', 'ND(PPM)',\n",
        "    'SM(PPM)', 'EU(PPM)', 'GD(PPM)', 'TB(PPM)', 'DY(PPM)', 'HO(PPM)',\n",
        "    'ER(PPM)', 'TM(PPM)', 'YB(PPM)', 'LU(PPM)', 'HF(PPM)', 'TA(PPM)',\n",
        "    'W(PPM)', 'AU(PPM)', 'HG(PPM)', 'TL(PPM)', 'PB(PPM)', 'BI(PPM)',\n",
        "    'TH(PPM)', 'U(PPM)', 'CR2O3(WT%)', 'BAO(WT%)', 'NIO(WT%)',\n",
        "            'LI2O(WT%)', 'RB2O(WT%)', 'K(WT%)']\n",
        "for col in _numericals:\n",
        "  df[col] = df[col].apply(convert_mixed_value)"
      ],
      "metadata": {
        "id": "4ypUgnQQ9MkJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Column Conversions"
      ],
      "metadata": {
        "id": "bcD7WE6ojXnL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Oxides"
      ],
      "metadata": {
        "id": "x226Ltmtjc5D"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Here, we convert each relevant column and will merge them in later code cells\n",
        "\n",
        "# We use .pipe() to chain functions\n",
        "# Converting WT% to WT% Oxides\n",
        "# SiO2\n",
        "df['SI(WT%)'] = df['SI(WT%)'].apply(Si)\n",
        "df['SI(PPM)'] = df['SI(PPM)'].pipe(ppm2wt).pipe(Si)\n",
        "\n",
        "# Al2O3\n",
        "df['AL(WT%)'] = df['AL(WT%)'].apply(Al)\n",
        "df['AL(PPM)'] = df['AL(PPM)'].pipe(ppm2wt).pipe(Al)\n",
        "\n",
        "# Fe2O3 (FeOT and Fe2O3T are the only ones measured with the T method)\n",
        "df['FE2O3(WT%)'] = df['FE2O3(WT%)'].apply(FeO)\n",
        "# Fe\n",
        "df['FE(WT%)'] = df['FE(WT%)'].apply(Fe)\n",
        "df['FE(PPM)'] = df['FE(PPM)'].pipe(ppm2wt).pipe(Fe)\n",
        "\n",
        "# MnO\n",
        "df['MN(PPM)'] = df['MN(PPM)'].pipe(ppm2wt).pipe(Mn)\n",
        "\n",
        "# MgO\n",
        "df['MG(WT%)'] = df['MG(WT%)'].apply(Mg)\n",
        "df['MG(PPM)'] = df['MG(PPM)'].pipe(ppm2wt).pipe(Mg)\n",
        "\n",
        "# CaO\n",
        "df['CA(PPM)'] = df['CA(PPM)'].pipe(ppm2wt).pipe(Ca)\n",
        "\n",
        "# Na2O\n",
        "df['NA(WT%)'] = df['NA(WT%)'].apply(Na)\n",
        "df['NA(PPM)'] = df['NA(PPM)'].pipe(ppm2wt).pipe(Na)\n",
        "\n",
        "# K2O\n",
        "df['K(WT%)'] = df['K(WT%)'].apply(K)\n",
        "df['K(PPM)'] = df['K(PPM)'].pipe(ppm2wt).pipe(K)\n",
        "\n",
        "# P2O5\n",
        "df['P(PPM)'] = df['P(PPM)'].pipe(ppm2wt).pipe(P)"
      ],
      "metadata": {
        "id": "3fHfyB9ed-uQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Non-Oxides"
      ],
      "metadata": {
        "id": "LIznz3fqjf06"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Trace elements; those in lower concentrations, should be left as ppm\n",
        "# MAJOR ELEMENTS - SiO2, TiO2, Al2O3, FeO, MnO, MgO, CaO, Na2O, K2O, P2O5\n",
        "\n",
        "# ZrO2 // ZR(PPM) - EP\n",
        "df['ZRO2(WT%)'] = df['ZRO2(WT%)'].pipe(ZRO2).pipe(wt2ppm)\n",
        "\n",
        "# HFO2 // HF(PPM) - FT\n",
        "df['HFO2(WT%)'] = df['HFO2(WT%)'].pipe(HFO2).pipe(wt2ppm)\n",
        "\n",
        "# THO2 // TH(PPM) - GD\n",
        "df['THO2(WT%)'] = df['THO2(WT%)'].pipe(THO2).pipe(wt2ppm)\n",
        "\n",
        "# UO2, UO3 // U(PPM), U(PPB) - GE, GF\n",
        "df['UO2(WT%)'] = df['UO2(WT%)'].pipe(UO2).pipe(wt2ppm)\n",
        "df['UO3(WT%)'] = df['UO3(WT%)'].pipe(UO3).pipe(wt2ppm)\n",
        "df['U(PPB)'] = df['U(PPB)'].pipe(ppb2m)\n",
        "\n",
        "# CR2O3 // CR(PPM) - DX\n",
        "df['CR2O3(WT%)'] = df['CR2O3(WT%)'].pipe(CR2O3).pipe(wt2ppm)\n",
        "df['CR(WT%)'] = df['CR(WT%)'].pipe(wt2ppm)\n",
        "\n",
        "# LA2O3 // LA(PPM) - FF\n",
        "df['LA2O3(WT%)'] = df['LA2O3(WT%)'].pipe(LA2O3).pipe(wt2ppm)\n",
        "\n",
        "# CE2O3 // CE(PPM) - FG\n",
        "df['CE2O3(WT%)'] = df['CE2O3(WT%)'].pipe(CE2O3).pipe(wt2ppm)\n",
        "\n",
        "# ND2O3 // ND(PPM) - FI\n",
        "df['ND2O3(WT%)'] = df['ND2O3(WT%)'].pipe(ND2O3).pipe(wt2ppm)\n",
        "\n",
        "# Y2O3 // Y(PPM) - EO\n",
        "df['Y2O3(WT%)'] = df['Y2O3(WT%)'].pipe(Y2O3).pipe(wt2ppm)\n",
        "\n",
        "# V2O3, V2O5 // V(PPM) - DW\n",
        "df['V2O3(WT%)'] = df['V2O3(WT%)'].pipe(V2O3).pipe(wt2ppm)\n",
        "df['V2O5(WT%)'] = df['V2O3(WT%)'].pipe(V2O5).pipe(wt2ppm)\n",
        "\n",
        "# NB2O3, NB2O5 // NB(PPM) - EQ\n",
        "df['NB2O3(WT%)'] = df['NB2O3(WT%)'].pipe(NB2O3).pipe(wt2ppm)\n",
        "df['NB2O5(WT%)'] = df['NB2O3(WT%)'].pipe(NB2O5).pipe(wt2ppm)\n",
        "\n",
        "# TA2O5 // TA(PPM) - FU\n",
        "df['TA2O5(WT%)'] = df['TA2O5(WT%)'].pipe(TA2O5).pipe(wt2ppm)\n",
        "\n",
        "# WO3 // W(PPM) - FV\n",
        "df['WO3(WT%)'] = df['WO3(WT%)'].pipe(WO3).pipe(wt2ppm)\n",
        "\n",
        "# BAO // BA(PPM) - FE\n",
        "df['BAO(WT%)'] = df['BAO(WT%)'].pipe(BAO).pipe(wt2ppm)\n",
        "\n",
        "# SRO // SR(PPM) - EN\n",
        "df['SRO(WT%)'] = df['SRO(WT%)'].pipe(SRO).pipe(wt2ppm)\n",
        "\n",
        "# PBO // PB(PPM) - FZ\n",
        "df['PBO(WT%)'] = df['PBO(WT%)'].pipe(PBO).pipe(wt2ppm)\n",
        "df['PB(PPB)'] = df['PB(PPB)'].pipe(ppb2m)\n",
        "\n",
        "# SNO2 // SN(PPM) - EZ\n",
        "df['SNO2(WT%)'] = df['SNO2(WT%)'].pipe(SNO2).pipe(wt2ppm)\n",
        "\n",
        "# NIO // NI(PPM) - EB\n",
        "df['NIO(WT%)'] = df['NIO(WT%)'].pipe(NIO).pipe(wt2ppm)\n",
        "\n",
        "# ZNO // ZN(PPM) - ED\n",
        "df['ZNO(WT%)'] = df['ZNO(WT%)'].pipe(ZNO).pipe(wt2ppm)\n",
        "\n",
        "# COO // CO(PPM) - EA\n",
        "df['COO(WT%)'] = df['COO(WT%)'].pipe(COO).pipe(wt2ppm)\n",
        "\n",
        "# CUO // CU(PPM) - EC\n",
        "df['CUO(WT%)'] = df['CUO(WT%)'].pipe(CUO).pipe(wt2ppm)\n",
        "\n",
        "# CS2O // CS(PPM) - FD\n",
        "df['CS2O(WT%)'] = df['CS2O(WT%)'].pipe(CS2O).pipe(wt2ppm)\n",
        "\n",
        "# RB2O // RB(PPM) - EM\n",
        "df['RB2O(WT%)'] = df['RB2O(WT%)'].pipe(RB2O).pipe(wt2ppm)\n",
        "\n",
        "# LI2O // LI(PPM) - CT\n",
        "df['LI2O(WT%)'] = df['LI2O(WT%)'].pipe(LI2O).pipe(wt2ppm)\n",
        "\n",
        "# F2O // F(PPM) - CX\n",
        "df['F(WT%)'] = df['F(WT%)'].pipe(wt2ppm)\n",
        "df['F2O(WT%)'] = df['F2O(WT%)'].pipe(F2O).pipe(wt2ppm)\n",
        "\n",
        "# CL2O // CL(PPM) - DF\n",
        "df['CL(WT%)'] = df['CL(WT%)'].pipe(wt2ppm)\n",
        "df['CL2O(WT%)'] = df['CL2O(WT%)'].pipe(CL2O).pipe(wt2ppm)"
      ],
      "metadata": {
        "id": "DcXneky4ODTY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Column Merging\n",
        "\n",
        "IMPORTANT - When merging the columns, data values need to stay **inplace**.\n",
        "This is because each value corresponds to measurements taken from specific rocks/minerals.\n",
        "\n",
        "Thus, we will use .fillna() with the 'inplace' argument to merge these columns."
      ],
      "metadata": {
        "id": "StWEc-Izjkep"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "### Oxides"
      ],
      "metadata": {
        "id": "ps3bdq8tOPbt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Oxides (We will leave major elements in WT% form)\n",
        "\n",
        "# SIO2\n",
        "df['SIO2(WT%)'] = df['SIO2(WT%)'].fillna(df['SI(PPM)']).fillna(df['SI(WT%)'])\n",
        "\n",
        "# AL2O3\n",
        "df['AL2O3(WT%)'] = df['AL2O3(WT%)'].fillna(df['AL(WT%)']).fillna(df['AL(PPM)'])\n",
        "\n",
        "# FEO\n",
        "df['FEO(WT%)'] = df['FEO(WT%)'].fillna(df['FE2O3(WT%)']).fillna(df['FE(WT%)']).fillna(df['FE(PPM)'])\n",
        "\n",
        "# MNO\n",
        "df['MNO(WT%)'] = df['MNO(WT%)'].fillna(df['MN(PPM)'])\n",
        "\n",
        "# MGO\n",
        "df['MGO(WT%)'] = df['MGO(WT%)'].fillna(df['MG(PPM)']).fillna(df['MG(WT%)'])\n",
        "\n",
        "# CAO\n",
        "df['CAO(WT%)'] = df['CAO(WT%)'].fillna(df['CA(PPM)'])\n",
        "\n",
        "# NA2O\n",
        "df['NA2O(WT%)'] = df['NA2O(WT%)'].fillna(df['NA(WT%)']).fillna(df['NA(PPM)'])\n",
        "\n",
        "# K20\n",
        "df['K2O(WT%)'] = df['K2O(WT%)'].fillna(df['K(WT%)']).fillna(df['K(PPM)'])\n",
        "\n",
        "# P2O5\n",
        "df['P2O5(WT%)'] = df['P2O5(WT%)'].fillna(df['P(PPM)'])"
      ],
      "metadata": {
        "id": "dVuMLDW0DuCh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Non-Oxides"
      ],
      "metadata": {
        "id": "VrAQZvMROSpR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Trace elements should be left in ppm format\n",
        "\n",
        "# ZR\n",
        "df['ZR(PPM)'] = df['ZR(PPM)'].fillna(df['ZRO2(WT%)'])\n",
        "\n",
        "# HF\n",
        "df['HF(PPM)'] = df['HF(PPM)'].fillna(df['HFO2(WT%)'])\n",
        "\n",
        "# TH\n",
        "df['TH(PPM)'] = df['TH(PPM)'].fillna(df['THO2(WT%)'])\n",
        "\n",
        "# U\n",
        "df['U(PPM)'] = df['U(PPM)'].fillna(df['UO2(WT%)']).fillna(df['UO3(WT%)']).fillna(df['U(PPB)'])\n",
        "\n",
        "# CR\n",
        "df['CR(PPM)'] = df['CR(PPM)'].fillna(df['CR2O3(WT%)']).fillna(df['CR(WT%)'])\n",
        "\n",
        "# LA\n",
        "df['LA(PPM)'] = df['LA(PPM)'].fillna(df['LA2O3(WT%)'])\n",
        "\n",
        "# CE\n",
        "df['CE(PPM)'] = df['CE(PPM)'].fillna(df['CE2O3(WT%)'])\n",
        "\n",
        "# ND\n",
        "df['ND(PPM)'] = df['ND(PPM)'].fillna(df['ND2O3(WT%)'])\n",
        "\n",
        "# Y\n",
        "df['Y(PPM)'] = df['Y(PPM)'].fillna(df['Y2O3(WT%)'])\n",
        "\n",
        "# V\n",
        "df['V(PPM)'] = df['V(PPM)'].fillna(df['V2O3(WT%)']).fillna(df['V2O5(WT%)'])\n",
        "\n",
        "# NB\n",
        "df['NB(PPM)'] = df['NB(PPM)'].fillna(df['NB2O3(WT%)']).fillna(df['NB2O5(WT%)'])\n",
        "\n",
        "# TA\n",
        "df['TA(PPM)'] = df['TA(PPM)'].fillna(df['TA2O5(WT%)'])\n",
        "\n",
        "# W\n",
        "df['W(PPM)'] = df['W(PPM)'].fillna(df['WO3(WT%)'])\n",
        "\n",
        "# BA\n",
        "df['BA(PPM)'] = df['BA(PPM)'].fillna(df['BAO(WT%)'])\n",
        "\n",
        "# SR\n",
        "df['SR(PPM)'] = df['SR(PPM)'].fillna(df['SRO(WT%)'])\n",
        "\n",
        "# PB\n",
        "df['PB(PPM)'] = df['PB(PPM)'].fillna(df['PBO(WT%)']).fillna(df['PB(PPB)'])\n",
        "\n",
        "# SN\n",
        "df['SN(PPM)'] = df['SN(PPM)'].fillna(df['SNO2(WT%)'])\n",
        "\n",
        "# NI\n",
        "df['NI(PPM)'] = df['NI(PPM)'].fillna(df['NIO(WT%)'])\n",
        "\n",
        "# ZN\n",
        "df['ZN(PPM)'] = df['ZN(PPM)'].fillna(df['ZNO(WT%)'])\n",
        "\n",
        "# CO\n",
        "df['CO(PPM)'] = df['CO(PPM)'].fillna(df['COO(WT%)'])\n",
        "\n",
        "# CU\n",
        "df['CU(PPM)'] = df['CU(PPM)'].fillna(df['CUO(WT%)'])\n",
        "\n",
        "# CS\n",
        "df['CS(PPM)'] = df['CS(PPM)'].fillna(df['CS2O(WT%)'])\n",
        "\n",
        "# RB\n",
        "df['RB(PPM)'] = df['RB(PPM)'].fillna(df['RB2O(WT%)'])\n",
        "\n",
        "# LI\n",
        "df['LI(PPM)'] = df['LI(PPM)'].fillna(df['LI2O(WT%)'])\n",
        "\n",
        "# F\n",
        "df['F(PPM)'] = df['F(PPM)'].fillna(df['F(WT%)']).fillna(df['F2O(WT%)'])\n",
        "\n",
        "# CL\n",
        "df['CL(PPM)'] = df['CL(PPM)'].fillna(df['CL(WT%)']).fillna(df['CL2O(WT%)'])\n"
      ],
      "metadata": {
        "id": "7nPGqRKhOTxx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Drop columns that have been merged"
      ],
      "metadata": {
        "id": "qk1HDxanTOOQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "postmerged = ['SI(WT%)', 'SI(PPM)', 'AL(WT%)', 'AL(PPM)', 'FE2O3(WT%)',\n",
        "              'FE2O3T(WT%)', 'FEOT(WT%)', 'MN(PPM)', 'MG(WT%)', 'MG(PPM)',\n",
        "              'CA(PPM)', 'NA(WT%)', 'NA(PPM)', 'K(WT%)', 'K(PPM)',\n",
        "              'P(PPM)', 'ZRO2(WT%)', 'HFO2(WT%)', 'THO2(WT%)', 'UO2(WT%)',\n",
        "              'UO3(WT%)', 'U(PPB)', 'CR2O3(WT%)', 'CR(WT%)', 'LA2O3(WT%)',\n",
        "              'CE2O3(WT%)', 'ND2O3(WT%)', 'Y2O3(WT%)', 'V2O3(WT%)', 'V2O5(WT%)',\n",
        "              'NB2O3(WT%)', 'NB2O5(WT%)', 'TA2O5(WT%)', 'WO3(WT%)', 'BAO(WT%)',\n",
        "              'SRO(WT%)', 'PBO(WT%)', 'PB(PPB)','SNO2(WT%)', 'NIO(WT%)',\n",
        "              'ZNO(WT%)', 'COO(WT%)', 'CUO(WT%)', 'CS2O(WT%)', 'RB2O(WT%)',\n",
        "              'LI2O(WT%)', 'F(WT%)', 'F2O(WT%)', 'CL(WT%)', 'CL2O(WT%)',\n",
        "              'FE(WT%)', 'FE(PPM)', 'AR(MOL/G)']"
      ],
      "metadata": {
        "id": "Tnva019kTmbS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df = df.drop(postmerged, axis = 1)\n",
        "df.columns"
      ],
      "metadata": {
        "id": "wayrAzWcWJAQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can now drop all rows containing NaNs from the LI(PPM) column as we have successfully merged them."
      ],
      "metadata": {
        "id": "sSY86Lrq5CyQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df = df.dropna(subset=['LI(PPM)'])\n",
        "df.reset_index(drop = True, inplace = True)\n",
        "df"
      ],
      "metadata": {
        "id": "1XLea7Tl5CLF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "####################################################################\n",
        "# Modelling\n"
      ],
      "metadata": {
        "id": "6dj0QBJfmaO-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "numericals = ['SIO2(WT%)', 'TIO2(WT%)', 'AL2O3(WT%)', 'FEO(WT%)', 'CAO(WT%)',\n",
        "       'MGO(WT%)', 'MNO(WT%)', 'K2O(WT%)', 'NA2O(WT%)', 'P2O5(WT%)', 'LI(PPM)',\n",
        "       'BE(PPM)', 'B(PPM)', 'F(PPM)', 'CL(PPM)', 'SC(PPM)',\n",
        "       'TI(PPM)', 'V(PPM)', 'CR(PPM)', 'CO(PPM)', 'NI(PPM)', 'CU(PPM)',\n",
        "       'ZN(PPM)', 'GA(PPM)', 'GE(PPM)', 'AS(PPM)', 'SE(PPM)',\n",
        "       'RB(PPM)', 'SR(PPM)', 'Y(PPM)', 'ZR(PPM)', 'NB(PPM)', 'MO(PPM)',\n",
        "       'AG(PPM)', 'CD(PPM)', 'IN(PPM)', 'SN(PPM)', 'SB(PPM)', 'TE(PPM)',\n",
        "       'CS(PPM)', 'BA(PPM)', 'LA(PPM)', 'CE(PPM)', 'PR(PPM)', 'ND(PPM)',\n",
        "       'SM(PPM)', 'EU(PPM)', 'GD(PPM)', 'TB(PPM)', 'DY(PPM)', 'HO(PPM)',\n",
        "       'ER(PPM)', 'TM(PPM)', 'YB(PPM)', 'LU(PPM)', 'HF(PPM)', 'TA(PPM)',\n",
        "       'W(PPM)', 'AU(PPM)', 'HG(PPM)', 'TL(PPM)', 'PB(PPM)', 'BI(PPM)',\n",
        "       'TH(PPM)', 'U(PPM)', 'LATITUDE (MIN.)', 'LATITUDE (MAX.)',\n",
        "       'LONGITUDE (MIN.)', 'LONGITUDE (MAX.)']\n",
        "\n",
        "#'ELEVATION (MIN.)','ELEVATION (MAX.)', 'DRILLING DEPTH (MIN.)',\n",
        "# 'DRILLING DEPTH (MAX.)'"
      ],
      "metadata": {
        "id": "yEuHwbVZp0xP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# from sklearn.preprocessing import MinMaxScaler\n",
        "# from sklearn.ensemble import RandomForestRegressor\n",
        "# from sklearn.metrics import mean_absolute_error\n",
        "# from sklearn.impute import SimpleImputer, KNNImputer\n",
        "\n",
        "# # Store results\n",
        "# results = {}\n",
        "\n",
        "# # Normalize the data\n",
        "# scaler = MinMaxScaler()\n",
        "# X_normalized = scaler.fit_transform(df[numericals])\n",
        "# X_normalized_df = pd.DataFrame(X_normalized, columns=numericals)\n",
        "\n",
        "# # Split data for evaluation\n",
        "# X_train, X_test, y_train, y_test = train_test_split(X_normalized_df, df['LI(PPM)'], test_size=0.3, random_state=42)\n",
        "\n",
        "# # Mean/Median Imputation\n",
        "# for strategy in ['mean', 'median']:\n",
        "#     imputer = SimpleImputer(strategy=strategy)\n",
        "#     X_train_imputed = imputer.fit_transform(X_train)\n",
        "#     X_test_imputed = imputer.transform(X_test)\n",
        "\n",
        "#     model = RandomForestRegressor()\n",
        "#     model.fit(X_train_imputed, y_train)\n",
        "#     predictions = model.predict(X_test_imputed)\n",
        "#     mae = mean_absolute_error(y_test, predictions)\n",
        "\n",
        "#     results[f'SimpleImputer ({strategy})'] = mae\n",
        "\n",
        "# # KNN Imputation\n",
        "# knn_imputer = KNNImputer(n_neighbors=2)\n",
        "# X_train_imputed = knn_imputer.fit_transform(X_train)\n",
        "# X_test_imputed = knn_imputer.transform(X_test)\n",
        "\n",
        "# model = RandomForestRegressor()\n",
        "# model.fit(X_train_imputed, y_train)\n",
        "# predictions = model.predict(X_test_imputed)\n",
        "# mae = mean_absolute_error(y_test, predictions)\n",
        "\n",
        "# results['KNNImputer'] = mae\n",
        "\n",
        "# ## Multiple (Iterative) Imputation\n",
        "# mice_imputer = IterativeImputer(random_state=42)\n",
        "# X_train_imputed = mice_imputer.fit_transform(X_train)\n",
        "# X_test_imputed = mice_imputer.transform(X_test)\n",
        "\n",
        "# model.fit(X_train_imputed, y_train)\n",
        "# predictions = model.predict(X_test_imputed)\n",
        "# mae = mean_absolute_error(y_test, predictions)\n",
        "# results['IterativeImputer'] = mae\n",
        "\n",
        "\n",
        "# # Display results\n",
        "# results_df = pd.DataFrame(list(results.items()), columns=['Imputation Method', 'MAE'])\n",
        "# print(results_df)\n",
        "\n",
        "# print(results_df.to_latex(index=False))\n"
      ],
      "metadata": {
        "id": "Dytq_A-X7LVG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Recategorization\n",
        "\n"
      ],
      "metadata": {
        "id": "Q56VMpS8-NK9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def categorize_rock(item):\n",
        "  if item == 'MONZOGRANITE':\n",
        "    return 'SYENITE'\n",
        "  elif item in ['RHYOLITE', 'RHYODACITE']:\n",
        "    return 'GRANITE'\n",
        "  elif 'GRANITE' in item:\n",
        "    return 'GRANITE'\n",
        "  elif item in ['APLITE', 'PEGMATITE', 'GREISEN']:\n",
        "    return 'GREISEN'\n",
        "  elif item in ['DACITE', 'TONALITE', 'TRONDHJEMITE']:\n",
        "    return 'GRANODIORITE'\n",
        "  elif 'GRANODIORITE' in item:\n",
        "    return 'GRANODIORITE'\n",
        "  elif 'DIORITE' in item:\n",
        "    return 'DIORITE'\n",
        "  elif item == 'ANDESITE':\n",
        "    return 'DIORITE'\n",
        "  elif 'GABBRO' in item:\n",
        "    return 'GABBRO'\n",
        "  elif 'SYENITE' in item:\n",
        "    return 'SYENITE'\n",
        "  elif item in ['MONZONITE', 'MELASYENITE', 'PHONOLITE', 'TRACHYTE']:\n",
        "    return 'SYENITE'\n",
        "  elif 'CARBONATITE' in item:\n",
        "    return 'CARBONATITE'\n",
        "  elif item in ['LAMPROPHYRE', 'VOGESITE', 'NEPHELINITE']:\n",
        "    return 'CARBONATITE'\n",
        "  elif 'PHLOGOPITE' in item:\n",
        "    return 'NOT GIVEN'\n",
        "  elif 'XENOLITH' in item:\n",
        "    return 'XENOLITH'\n",
        "  elif item in ['CLINOPYROXENITE', 'HARZBURGITE', 'WEHRLITE', 'ECLOGITE']:\n",
        "    return 'XENOLITH'\n",
        "  else:\n",
        "    return item\n",
        "\n",
        "# clinopyroxenite, harzburgite, wehrlite, eclogite"
      ],
      "metadata": {
        "id": "WfHLWAsZGk_6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Cdf = df.copy()\n",
        "Cdf['ROCK NAME'] = Cdf['ROCK NAME'].apply(categorize_rock)"
      ],
      "metadata": {
        "id": "cExkw1Xg3WGK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Cdf['ROCK NAME'].value_counts()"
      ],
      "metadata": {
        "id": "pW4s7cAh3O1t"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Shifting the latitude columns\n",
        "Cdf['LATITUDE (MIN.)'] = Cdf['LATITUDE (MIN.)'] + 90\n",
        "Cdf['LATITUDE (MAX.)'] = Cdf['LATITUDE (MAX.)'] + 90\n",
        "\n",
        "# Shifting the longitude columns\n",
        "Cdf['LONGITUDE (MIN.)'] = Cdf['LONGITUDE (MIN.)'] + 180\n",
        "Cdf['LONGITUDE (MAX.)'] = Cdf['LONGITUDE (MAX.)'] + 180\n"
      ],
      "metadata": {
        "id": "89eBqS0o8naP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Rectify negatives\n",
        "def rectify_negatives(x):\n",
        "    return x if x >= 0 else 1e-9\n",
        "\n",
        "# Apply the function to the columns with negative values\n",
        "columns_with_negatives = ['LI(PPM)', 'LA(PPM)',\n",
        "                          'PR(PPM)', 'ND(PPM)']  # Replace with the actual column names\n",
        "for col in columns_with_negatives:\n",
        "    Cdf[col] = Cdf[col].apply(rectify_negatives)\n"
      ],
      "metadata": {
        "id": "1dfmpZz79zVv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Let us split the dataset first\n",
        "arbitrary = ['LOCATION COMMENT',\n",
        "              'LOCATION', 'SPOT', 'CRYSTAL',\n",
        "             'RIM/CORE (MINERAL GRAINS)',\n",
        "             'ALTERATION', 'ELEVATION (MIN.)','ELEVATION (MAX.)',\n",
        "             'DRILLING DEPTH (MIN.)', 'DRILLING DEPTH (MAX.)',\n",
        "             'ROCK TEXTURE']\n",
        "\n",
        "Mdf = Cdf.drop(arbitrary, axis = 1)\n",
        "Mdf.columns"
      ],
      "metadata": {
        "id": "6dFNtQG2tRhk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Data Imputation (KNN)"
      ],
      "metadata": {
        "id": "Z7YTyYyUx21o"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Create the KNN imputer instance\n",
        "imputer = KNNImputer(n_neighbors=5)  # You can adjust the number of neighbors if needed\n",
        "\n",
        "# Apply imputation on the numerical columns of the dataset\n",
        "Mdf[numericals] = imputer.fit_transform(Mdf[numericals])"
      ],
      "metadata": {
        "id": "dSx2JlwF0bax"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# One-Hot Encoding"
      ],
      "metadata": {
        "id": "QaDeco1MtmBk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# One hot encoding\n",
        "MCat = ['TECTONIC SETTING', 'ROCK NAME', 'PRIMARY/SECONDARY',\n",
        "                'LAND/SEA (SAMPLING)', 'MINERAL']\n",
        "# one-hot encode each column\n",
        "for col in MCat:\n",
        "  one_hot = pd.get_dummies(Mdf[col], prefix=col)\n",
        "# Concatenate the one-hot encoded column back into the original df\n",
        "  Mdf = pd.concat([Mdf, one_hot], axis = 1)\n",
        "  Mdf = Mdf.drop(col, axis=1)"
      ],
      "metadata": {
        "id": "P9gAA2QPtUee"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Normalization\n",
        "\n"
      ],
      "metadata": {
        "id": "YQkSbgFZv0HB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Splitting the dataset into train and test\n",
        "Mdf_train, Mdf_test = train_test_split(Mdf, test_size=0.3, random_state=42)\n",
        "\n",
        "# 1. Z-Score Normalization\n",
        "scaler = StandardScaler()\n",
        "Mdf_train_zscore = Mdf_train.copy()\n",
        "Mdf_test_zscore = Mdf_test.copy()\n",
        "Mdf_train_zscore[numericals] = scaler.fit_transform(Mdf_train[numericals])\n",
        "Mdf_test_zscore[numericals] = scaler.transform(Mdf_test[numericals])\n",
        "\n",
        "# 2. Log Transformation Normalization (Note: adding a small constant to avoid log(0))\n",
        "Mdf_train_log = Mdf_train.copy()\n",
        "Mdf_test_log = Mdf_test.copy()\n",
        "Mdf_train_log[numericals] = Mdf_train[numericals].applymap(lambda x: np.log(x + 1e-9))\n",
        "Mdf_test_log[numericals] = Mdf_test[numericals].applymap(lambda x: np.log(x + 1e-9))\n",
        "\n",
        "# 3. Square Root Normalization\n",
        "Mdf_train_sqrt = Mdf_train.copy()\n",
        "Mdf_test_sqrt = Mdf_test.copy()\n",
        "Mdf_train_sqrt[numericals] = np.sqrt(Mdf_train[numericals])\n",
        "Mdf_test_sqrt[numericals] = np.sqrt(Mdf_test[numericals])\n",
        "\n"
      ],
      "metadata": {
        "id": "AExhHV4zv4KI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Train/Test Sets:\n",
        "\n",
        "* Mdf_train_zscore, Mdf_test_zscore\n",
        "\n",
        "* Mdf_train_log, Mdf_test_log\n",
        "\n",
        "* Mdf_train_sqrt, Mdf_test_sqrt"
      ],
      "metadata": {
        "id": "Q4H5tgoc452b"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "####################################################################\n",
        "\n",
        "# Models"
      ],
      "metadata": {
        "id": "tkOD6kzVrX6Q"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Random Forest"
      ],
      "metadata": {
        "id": "3IYbVnVg4CrN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "from sklearn.metrics import mean_squared_error\n",
        "\n",
        "# Define a function to run Random Forest and plot scatter\n",
        "def run_rf_and_plot(X_train, y_train, X_test, y_test, title):\n",
        "    rf = RandomForestRegressor(n_estimators=100, random_state=42)\n",
        "    rf.fit(X_train, y_train)\n",
        "    y_pred = rf.predict(X_test)\n",
        "    mse = mean_squared_error(y_test, y_pred)\n",
        "\n",
        "    # Scatter plot of Predicted vs Actual\n",
        "    plt.scatter(y_test, y_pred)\n",
        "    plt.xlabel('True Values')\n",
        "    plt.ylabel('Predictions')\n",
        "    plt.title(title + ' (Random Forest)')\n",
        "\n",
        "    # Add line of best fit\n",
        "    z = np.polyfit(y_test, y_pred, 1)\n",
        "    p = np.poly1d(z)\n",
        "    plt.plot(y_test, p(y_test), \"r--\", label='Line of Best Fit')\n",
        "\n",
        "    # Save the plot\n",
        "    plt.savefig(f\"{title}.png\")\n",
        "    plt.show()\n",
        "\n",
        "    return mse\n",
        "\n",
        "# List of train-test sets\n",
        "datasets = {\n",
        "    \"Z-Score\": (Mdf_train_zscore, Mdf_test_zscore),\n",
        "    \"Log\": (Mdf_train_log, Mdf_test_log),\n",
        "    \"Square Root\": (Mdf_train_sqrt, Mdf_test_sqrt)\n",
        "}\n",
        "\n",
        "# Store results\n",
        "results = {}\n",
        "\n",
        "# Loop through each dataset and apply Random Forest\n",
        "for name, (train, test) in datasets.items():\n",
        "    X_train = train.drop(\"LI(PPM)\", axis=1)\n",
        "    y_train = train[\"LI(PPM)\"]\n",
        "    X_test = test.drop(\"LI(PPM)\", axis=1)\n",
        "    y_test = test[\"LI(PPM)\"]\n",
        "\n",
        "    mse = run_rf_and_plot(X_train, y_train, X_test, y_test, f\"{name} Normalization MSE Scatter Plot\")\n",
        "    results[name] = mse\n"
      ],
      "metadata": {
        "id": "04TrsNnssCVv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "from sklearn.metrics import mean_squared_error\n",
        "\n",
        "# Define a function to run Random Forest and plot scatter\n",
        "def run_rf_and_plot(X_train, y_train, X_test, y_test, title):\n",
        "    rf = RandomForestRegressor(n_estimators=100, random_state=42)\n",
        "    rf.fit(X_train, y_train)\n",
        "    y_pred = rf.predict(X_test)\n",
        "    mse = r2_score(y_test, y_pred)\n",
        "\n",
        "    # Scatter plot of Predicted vs Actual\n",
        "    plt.scatter(y_test, y_pred)\n",
        "    plt.xlabel('True Values')\n",
        "    plt.ylabel('Predictions')\n",
        "    plt.title(title + ' (Random Forest)')\n",
        "    # Save the plot\n",
        "    plt.savefig(f\"{title}.png\")\n",
        "    plt.show()\n",
        "\n",
        "    return mse\n",
        "\n",
        "# List of train-test sets\n",
        "datasets = {\n",
        "    \"Z-Score\": (Mdf_train_zscore, Mdf_test_zscore),\n",
        "    \"Log\": (Mdf_train_log, Mdf_test_log),\n",
        "    \"Square Root\": (Mdf_train_sqrt, Mdf_test_sqrt)\n",
        "}\n",
        "\n",
        "# Store results\n",
        "results = {}\n",
        "\n",
        "# Loop through each dataset and apply Random Forest\n",
        "for name, (train, test) in datasets.items():\n",
        "    X_train = train.drop(\"LI(PPM)\", axis=1)\n",
        "    y_train = train[\"LI(PPM)\"]\n",
        "    X_test = test.drop(\"LI(PPM)\", axis=1)\n",
        "    y_test = test[\"LI(PPM)\"]\n",
        "\n",
        "    mse = run_rf_and_plot(X_train, y_train, X_test, y_test, f\"{name} Normalization MSE Scatter Plot\")\n",
        "    results[name] = mse\n"
      ],
      "metadata": {
        "id": "eYFMbIlx6bHU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Convert the results dictionary into a DataFrame\n",
        "mse_df = pd.DataFrame(list(results.items()), columns=['Normalization Method', 'MSE'])\n",
        "\n",
        "# Print the DataFrame for verification\n",
        "print(mse_df)\n",
        "\n",
        "# To convert DataFrame to LaTeX\n",
        "latex_code = mse_df.to_latex(index=False)\n",
        "print(latex_code)\n"
      ],
      "metadata": {
        "id": "PVdcQz5LVmsI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# from sklearn.ensemble import RandomForestRegressor\n",
        "# from sklearn.metrics import mean_absolute_error  # Importing mean_absolute_error instead of mean_squared_error\n",
        "\n",
        "# # Define a function to run Random Forest\n",
        "# def run_rf(X_train, y_train, X_test, y_test):\n",
        "#     rf = RandomForestRegressor(n_estimators=100, random_state=42)\n",
        "#     rf.fit(X_train, y_train)\n",
        "#     y_pred = rf.predict(X_test)\n",
        "#     return mean_absolute_error(y_test, y_pred)  # Changed this line to return MAE\n",
        "\n",
        "# # List of train-test sets\n",
        "# datasets = {\n",
        "#     \"Z-Score\": (Mdf_train_zscore, Mdf_test_zscore),\n",
        "#     \"Log\": (Mdf_train_log, Mdf_test_log),\n",
        "#     \"Square Root\": (Mdf_train_sqrt, Mdf_test_sqrt)\n",
        "# }\n",
        "\n",
        "# # Store results\n",
        "# results = {}\n",
        "\n",
        "# # Loop through each dataset and apply Random Forest\n",
        "# for name, (train, test) in datasets.items():\n",
        "#     X_train = train.drop(\"LI(PPM)\", axis=1)\n",
        "#     y_train = train[\"LI(PPM)\"]\n",
        "#     X_test = test.drop(\"LI(PPM)\", axis=1)\n",
        "#     y_test = test[\"LI(PPM)\"]\n",
        "\n",
        "#     mae = run_rf(X_train, y_train, X_test, y_test)  # Storing MAE instead of MSE\n",
        "#     results[name] = mae  # Storing MAE instead of MSE\n",
        "#     print(f\"{name} Normalization MAE: {mae}\")  # Changed this line to print MAE\n"
      ],
      "metadata": {
        "id": "I_dkUkC3fPh8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Neural Network"
      ],
      "metadata": {
        "id": "GBu4t_5R4LzX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "# Define a function to train a neural network\n",
        "def run_nn(X_train, y_train, X_test, y_test, epochs=50):\n",
        "    model = tf.keras.Sequential([\n",
        "        tf.keras.layers.Dense(128, activation = 'relu', input_shape=(X_train.shape[1],)),\n",
        "        Dropout(0.2),  # Add dropout with rate 0.2\n",
        "        tf.keras.layers.Dense(64, activation = 'relu'),\n",
        "        Dropout(0.2),  # Add dropout with rate 0.2\n",
        "        tf.keras.layers.Dense(32, activation = 'relu'),\n",
        "        Dropout(0.2),  # Add dropout with rate 0.2\n",
        "        tf.keras.layers.Dense(16, activation = 'relu'),\n",
        "        Dropout(0.2),  # Add dropout with rate 0.2\n",
        "        tf.keras.layers.Dense(1, activation = 'linear')\n",
        "    ])\n",
        "\n",
        "    model.compile(loss='mean_squared_error',\n",
        "                  optimizer=tf.keras.optimizers.Adam(learning_rate=0.001),\n",
        "                  metrics=['mse'])\n",
        "\n",
        "    history = model.fit(X_train, y_train, epochs=epochs, validation_split=0.2, verbose=0)\n",
        "    eval_metrics = model.evaluate(X_test, y_test, verbose=0)\n",
        "    return model, eval_metrics, history\n"
      ],
      "metadata": {
        "id": "k4OqtuuUnNnf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define a function to train a neural network\n",
        "def run_nn(X_train, y_train, X_test, y_test, epochs=50):\n",
        "    model = tf.keras.Sequential([\n",
        "        tf.keras.layers.Dense(128, activation='relu', input_shape=(X_train.shape[1],)),\n",
        "        Dropout(0.2),  # Add dropout with rate 0.2\n",
        "        tf.keras.layers.Dense(64, activation='relu'),\n",
        "        Dropout(0.2),  # Add dropout with rate 0.2\n",
        "        tf.keras.layers.Dense(32, activation='relu'),\n",
        "        Dropout(0.2),  # Add dropout with rate 0.2\n",
        "        tf.keras.layers.Dense(16, activation='relu'),\n",
        "        Dropout(0.2),  # Add dropout with rate 0.2\n",
        "        tf.keras.layers.Dense(1, activation='linear')\n",
        "    ])\n",
        "\n",
        "    model.compile(loss='mean_squared_error',\n",
        "                  optimizer=tf.keras.optimizers.Adam(learning_rate=0.001),\n",
        "                  metrics=['mse'])\n",
        "\n",
        "    history = model.fit(X_train, y_train, epochs=epochs, validation_split=0.2, verbose=0)\n",
        "\n",
        "    # Get predictions on the test set\n",
        "    y_pred = model.predict(X_test).flatten()\n",
        "\n",
        "    # Calculate R^2 score\n",
        "    r2 = r2_score(y_test, y_pred)\n",
        "\n",
        "    eval_metrics = model.evaluate(X_test, y_test, verbose=0)\n",
        "\n",
        "    return model, eval_metrics, history, r2\n",
        "\n",
        "# Your code for running the NN models\n",
        "nn_results = {}\n",
        "\n",
        "for name, (train, test) in datasets.items():\n",
        "    X_train = train.drop(\"LI(PPM)\", axis=1)\n",
        "    y_train = train[\"LI(PPM)\"]\n",
        "    X_test = test.drop(\"LI(PPM)\", axis=1)\n",
        "    y_test = test[\"LI(PPM)\"]\n",
        "\n",
        "    model, eval_metrics, history, r2 = run_nn(X_train, y_train, X_test, y_test)\n",
        "    nn_results[name] = {'MSE': eval_metrics[0], 'R2': r2}  # Storing both MSE and R^2\n",
        "\n",
        "# Convert the results dictionary into a DataFrame\n",
        "nn_results_df = pd.DataFrame.from_dict(nn_results, orient='index').reset_index()\n",
        "nn_results_df.columns = ['Normalization Method', 'MSE', 'R2']\n",
        "\n",
        "# Print the DataFrame for verification\n",
        "print(nn_results_df)\n",
        "\n",
        "# To convert DataFrame to LaTeX\n",
        "nn_latex_code = nn_results_df.to_latex(index=False)\n",
        "print(nn_latex_code)\n"
      ],
      "metadata": {
        "id": "hqmNNoXPKBDf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# # List of train-test sets\n",
        "# datasets = {\n",
        "#     \"Z-Score\": (Mdf_train_zscore.drop(\"LI(PPM)\", axis=1), Mdf_train_zscore[\"LI(PPM)\"], Mdf_test_zscore.drop(\"LI(PPM)\", axis=1), Mdf_test_zscore[\"LI(PPM)\"]),\n",
        "#     \"Log\": (Mdf_train_log.drop(\"LI(PPM)\", axis=1), Mdf_train_log[\"LI(PPM)\"], Mdf_test_log.drop(\"LI(PPM)\", axis=1), Mdf_test_log[\"LI(PPM)\"]),\n",
        "#     \"Square Root\": (Mdf_train_sqrt.drop(\"LI(PPM)\", axis=1), Mdf_train_sqrt[\"LI(PPM)\"], Mdf_test_sqrt.drop(\"LI(PPM)\", axis=1), Mdf_test_sqrt[\"LI(PPM)\"])\n",
        "# }\n",
        "\n",
        "# # Store results\n",
        "# nn_results = {}\n",
        "\n",
        "# # Loop through each dataset and apply Neural Network\n",
        "# for name, (X_train, y_train, X_test, y_test) in datasets.items():\n",
        "#     model, eval_metrics, history = run_nn(X_train, y_train, X_test, y_test)\n",
        "#     nn_results[name] = eval_metrics[1]  # Store the MSE\n",
        "#     print(f\"{name} Normalization Neural Network MSE: {eval_metrics[1]}\")\n"
      ],
      "metadata": {
        "id": "8ZTrGnSy4ko9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Your code for running the NN models\n",
        "nn_results = {}\n",
        "\n",
        "for name, (train, test) in datasets.items():\n",
        "    X_train = train.drop(\"LI(PPM)\", axis=1)\n",
        "    y_train = train[\"LI(PPM)\"]\n",
        "    X_test = test.drop(\"LI(PPM)\", axis=1)\n",
        "    y_test = test[\"LI(PPM)\"]\n",
        "\n",
        "    model, eval_metrics, history = run_nn(X_train, y_train, X_test, y_test)\n",
        "    nn_results[name] = eval_metrics[0]  # Assuming that MSE is the first element in eval_metrics\n",
        "import pandas as pd\n",
        "\n",
        "# Convert the results dictionary into a DataFrame\n",
        "nn_mse_df = pd.DataFrame(list(nn_results.items()), columns=['Normalization Method', 'MSE'])\n",
        "\n",
        "# Print the DataFrame for verification\n",
        "print(nn_mse_df)\n",
        "\n",
        "# To convert DataFrame to LaTeX\n",
        "nn_latex_code = nn_mse_df.to_latex(index=False)\n",
        "print(nn_latex_code)"
      ],
      "metadata": {
        "id": "8V-APgRaWssL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import tensorflow as tf\n",
        "\n",
        "# Your existing datasets (replace these with your actual datasets)\n",
        "datasets = {\n",
        "    \"Z-Score\": (Mdf_train_zscore, Mdf_test_zscore),\n",
        "    \"Log\": (Mdf_train_log, Mdf_test_log),\n",
        "    \"Square Root\": (Mdf_train_sqrt, Mdf_test_sqrt)\n",
        "}\n",
        "\n",
        "# Initialize histories dictionary\n",
        "histories = {}\n",
        "\n",
        "# Run the model for each dataset and collect history\n",
        "for name, (train, test) in datasets.items():\n",
        "    X_train = train.drop(\"LI(PPM)\", axis=1)\n",
        "    y_train = train[\"LI(PPM)\"]\n",
        "    X_test = test.drop(\"LI(PPM)\", axis=1)\n",
        "    y_test = test[\"LI(PPM)\"]\n",
        "\n",
        "    # Running the neural network model\n",
        "    _, _, history = run_nn(X_train, y_train, X_test, y_test)\n",
        "\n",
        "    # Storing the history\n",
        "    histories[name] = history\n",
        "\n",
        "# Plotting the MSE curves\n",
        "plt.figure(figsize=(10, 6))\n",
        "\n",
        "for name, history in histories.items():\n",
        "    plt.plot(history.history['mse'], label=f\"{name} Train MSE\")\n",
        "    plt.plot(history.history['val_mse'], label=f\"{name} Validation MSE\")\n",
        "\n",
        "plt.title('Neural Network MSE Curves')\n",
        "plt.ylabel('MSE')\n",
        "plt.xlabel('Epoch')\n",
        "plt.legend(loc='upper right')\n",
        "plt.savefig('NNMSE.png')\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "vqf1PxpjYz_h"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import tensorflow as tf\n",
        "\n",
        "# Your existing datasets (replace these with your actual datasets)\n",
        "datasets = {\n",
        "    \"Z-Score\": (Mdf_train_zscore, Mdf_test_zscore),\n",
        "    \"Log\": (Mdf_train_log, Mdf_test_log),\n",
        "    \"Square Root\": (Mdf_train_sqrt, Mdf_test_sqrt)\n",
        "}\n",
        "\n",
        "# Initialize histories dictionary\n",
        "histories = {}\n",
        "\n",
        "# Mapping for custom legend names\n",
        "legend_mapping = {\n",
        "    \"Z-Score\": \"M.1.1\",\n",
        "    \"Log\": \"M.1.2\",\n",
        "    \"Square Root\": \"M.1.3\"\n",
        "}\n",
        "\n",
        "# Run the model for each dataset and collect history\n",
        "for name, (train, test) in datasets.items():\n",
        "    X_train = train.drop(\"LI(PPM)\", axis=1)\n",
        "    y_train = train[\"LI(PPM)\"]\n",
        "    X_test = test.drop(\"LI(PPM)\", axis=1)\n",
        "    y_test = test[\"LI(PPM)\"]\n",
        "\n",
        "    # Running the neural network model\n",
        "    _, _, history = run_nn(X_train, y_train, X_test, y_test)\n",
        "\n",
        "    # Storing the history\n",
        "    histories[name] = history\n",
        "\n",
        "# Plotting the MSE curves\n",
        "plt.figure(figsize=(10, 6))\n",
        "\n",
        "for name, history in histories.items():\n",
        "    plt.plot(history.history['mse'], label=f\"{legend_mapping[name]} Train MSE\")\n",
        "    plt.plot(history.history['val_mse'], label=f\"{legend_mapping[name]} Validation MSE\")\n",
        "\n",
        "plt.title('Neural Network MSE Curves')\n",
        "plt.ylabel('MSE')\n",
        "plt.xlabel('Epoch')\n",
        "plt.legend(loc='upper right')\n",
        "plt.savefig('NNMSE.png')\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "xWv3esjpvwbI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Multivariate Polynomial Regression\n"
      ],
      "metadata": {
        "id": "pbw9BcPB4OX7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Function to run Polynomial Regression and plot\n",
        "def run_polyreg_and_plot(X_train, y_train, X_test, y_test, degree=2, alpha=1.0, title=''):\n",
        "    polyreg = make_pipeline(PolynomialFeatures(degree), Ridge(alpha=alpha))\n",
        "    polyreg.fit(X_train, y_train)\n",
        "    y_pred = polyreg.predict(X_test)\n",
        "    mse = r2_score(y_test, y_pred)\n",
        "\n",
        "    # Scatter plot of Predicted vs Actual\n",
        "    plt.figure()\n",
        "    plt.scatter(y_test, y_pred, color='purple')\n",
        "    plt.xlabel('True Values')\n",
        "    plt.ylabel('Predictions')\n",
        "    plt.title(title + ' (Polynomial Regression)')\n",
        "\n",
        "    # Save the plot\n",
        "    plt.savefig(f\"{title}.png\")\n",
        "    plt.show()\n",
        "\n",
        "    return mse\n",
        "\n",
        "# Loop through each dataset and apply Polynomial Regression\n",
        "polyreg_results = {}\n",
        "for name, (train, test) in datasets.items():\n",
        "    X_train = train.drop(\"LI(PPM)\", axis=1)\n",
        "    y_train = train[\"LI(PPM)\"]\n",
        "    X_test = test.drop(\"LI(PPM)\", axis=1)\n",
        "    y_test = test[\"LI(PPM)\"]\n",
        "\n",
        "    mse = run_polyreg_and_plot(X_train, y_train, X_test, y_test, title=f\"{name} Normalization MSE Scatter Plot\")\n",
        "    polyreg_results[name] = mse\n",
        "    print(f\"{name} Normalization MSE: {mse}\")\n"
      ],
      "metadata": {
        "id": "zwOE-14Z4YRV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Create a DataFrame\n",
        "mse_table = pd.DataFrame(list(polyreg_results.items()), columns=['Normalization', 'MSE'])\n",
        "\n",
        "# To display the table in Python, just print it\n",
        "print(mse_table)\n",
        "\n",
        "# To export to LaTeX\n",
        "print(mse_table.to_latex(index=False))\n"
      ],
      "metadata": {
        "id": "ElLyikMiaEJV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# XGBoost"
      ],
      "metadata": {
        "id": "TEvf1F9F4Rf4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import xgboost as xgb\n",
        "from sklearn.metrics import mean_squared_error\n",
        "\n",
        "# Function to run XGBoost and plot scatter\n",
        "def run_xgb_and_plot(X_train, y_train, X_test, y_test, title, n_estimators=100, learning_rate=0.1):\n",
        "    xgb_model = xgb.XGBRegressor(objective='reg:squarederror', n_estimators=n_estimators, learning_rate=learning_rate)\n",
        "    xgb_model.fit(X_train, y_train)\n",
        "    y_pred = xgb_model.predict(X_test)\n",
        "    mse = r2_score(y_test, y_pred)\n",
        "\n",
        "    # Scatter plot of Predicted vs Actual\n",
        "    plt.figure()\n",
        "    plt.scatter(y_test, y_pred, color='red')\n",
        "    plt.xlabel('True Values')\n",
        "    plt.ylabel('Predictions')\n",
        "    plt.title(title)\n",
        "\n",
        "    # Save the plot\n",
        "    plt.savefig(f\"{title}.png\")\n",
        "\n",
        "    # Show the plot\n",
        "    plt.show()\n",
        "\n",
        "    return mse\n",
        "\n",
        "# List of train-test sets (replace these with your actual datasets)\n",
        "datasets = {\n",
        "    \"Z-Score\": (Mdf_train_zscore, Mdf_test_zscore),\n",
        "    \"Log\": (Mdf_train_log, Mdf_test_log),\n",
        "    \"Square Root\": (Mdf_train_sqrt, Mdf_test_sqrt)\n",
        "}\n",
        "\n",
        "# Store results\n",
        "xgb_results = {}\n",
        "\n",
        "# Loop through each dataset and apply XGBoost\n",
        "for name, (train, test) in datasets.items():\n",
        "    X_train = train.drop(\"LI(PPM)\", axis=1)\n",
        "    y_train = train[\"LI(PPM)\"]\n",
        "    X_test = test.drop(\"LI(PPM)\", axis=1)\n",
        "    y_test = test[\"LI(PPM)\"]\n",
        "\n",
        "    mse = run_xgb_and_plot(X_train, y_train, X_test, y_test, f\"{name} Normalization XGBoost MSE Scatter Plot\")\n",
        "    xgb_results[name] = mse\n",
        "    print(f\"{name} Normalization XGBoost MSE: {mse}\")\n"
      ],
      "metadata": {
        "id": "oIZxKrKu4cDr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Generating LaTeX table for the MSE\n",
        "latex_table = \"\\\\begin{tabular}{|c|c|}\\n\\\\hline\\nNormalization & MSE \\\\\\\\\\n\\\\hline\"\n",
        "\n",
        "for name, mse in xgb_results.items():\n",
        "    latex_table += f\"\\n{name} & {mse:.4f} \\\\\\\\\\n\\\\hline\"\n",
        "\n",
        "latex_table += \"\\\\end{tabular}\"\n",
        "print(latex_table)\n"
      ],
      "metadata": {
        "id": "beTgWeEjcayk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Distributions before and after"
      ],
      "metadata": {
        "id": "dHGl4y25zcqX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Features to examine\n",
        "features_to_plot = ['AL2O3(WT%)', 'MGO(WT%)']\n",
        "\n",
        "# Prepare datasets\n",
        "all_datasets = {\n",
        "    'Original': Mdf,\n",
        "    'Z-Score': Mdf_train_zscore,  # Assuming you normalized the whole dataset\n",
        "    'Log': Mdf_train_log,         # Assuming you normalized the whole dataset\n",
        "    'Square Root': Mdf_train_sqrt # Assuming you normalized the whole dataset\n",
        "}\n",
        "\n",
        "# Loop through each feature\n",
        "for feature in features_to_plot:\n",
        "    plt.figure(figsize=(12, 8))\n",
        "\n",
        "    # Loop through each dataset\n",
        "    for name, dataset in all_datasets.items():\n",
        "        sns.kdeplot(dataset[feature], label=f\"{name}\", shade=True)\n",
        "\n",
        "    plt.title(f'Density Plot of {feature} After Different Normalizations')\n",
        "    plt.xlabel(f'{feature}')\n",
        "    plt.ylabel('Density')\n",
        "    plt.legend()\n",
        "    plt.savefig(f'BeforexAfter{feature}.png')\n",
        "    plt.show()\n"
      ],
      "metadata": {
        "id": "SdGGYs78zflk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "############################################################\n",
        "# Feature Engineering Tests"
      ],
      "metadata": {
        "id": "56YIMRFvABjd"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Z Normalized"
      ],
      "metadata": {
        "id": "FUlGbc6O05Ax"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# M2.1 Z Normalized, X imputation\n",
        "M2_1_train = Mdf.sample(frac=0.8, random_state=42)\n",
        "M2_1_val = Mdf.drop(M2_1_train.index)\n",
        "\n",
        "# Handle missing values before normalization\n",
        "M2_1imputer = SimpleImputer(strategy='mean')\n",
        "\n",
        "# Perform imputation on training numerical features\n",
        "M2_1imputed_data = M2_1imputer.fit_transform(M2_1_train[numericals])\n",
        "M2_1_train[numericals] = pd.DataFrame(M2_1imputed_data, columns=numericals, index=M2_1_train.index)\n",
        "\n",
        "# Perform imputation on validation numerical features with the same imputer\n",
        "M2_1imputed_data = M2_1imputer.transform(M2_1_val[numericals])\n",
        "M2_1_val[numericals] = pd.DataFrame(M2_1imputed_data, columns=numericals, index=M2_1_val.index)\n",
        "\n",
        "# Normalize the numerical features using StandardScaler for Z-score normalization\n",
        "M2_1scaler = StandardScaler()\n",
        "\n",
        "M2_1_train[numericals] = M2_1scaler.fit_transform(M2_1_train[numericals])\n",
        "M2_1_val[numericals] = M2_1scaler.transform(M2_1_val[numericals])\n",
        "\n",
        "# Split the labels from the rest of the features\n",
        "M2_1train_features = M2_1_train.copy()\n",
        "M2_1val_features = M2_1_val.copy()\n",
        "\n",
        "M2_1train_features = M2_1train_features.astype('float32')\n",
        "M2_1val_features = M2_1val_features.astype('float32')\n",
        "\n",
        "M2_1train_labels = M2_1train_features.pop('LI(PPM)')\n",
        "M2_1val_labels = M2_1val_features.pop('LI(PPM)')\n"
      ],
      "metadata": {
        "id": "ajBg88au1doE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# MODEL 2"
      ],
      "metadata": {
        "id": "2QLYNXoE2trW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Normalized\n",
        "model2 = tf.keras.Sequential([\n",
        "    tf.keras.layers.Dense(128, activation = 'relu', input_shape=(M2_1train_features.shape[1],)),\n",
        "    tf.keras.layers.Dense(64, activation = 'relu'),\n",
        "    tf.keras.layers.Dense(32, activation = 'relu'),\n",
        "    tf.keras.layers.Dense(16, activation = 'relu'),\n",
        "    tf.keras.layers.Dense(1, activation = 'linear')\n",
        "])\n",
        "\n",
        "model2.compile(loss='mean_squared_error', # MSE because we are predicting a continuous value\n",
        "                   optimizer = tf.keras.optimizers.Adam(learning_rate=0.001),\n",
        "                      metrics = ['mse'])\n",
        "\n",
        "history2_1 = model2.fit(M2_1train_features, M2_1train_labels, epochs=30,\n",
        "                  batch_size = 32, validation_data = (M2_1val_features, M2_1val_labels))"
      ],
      "metadata": {
        "id": "vvw2_Nju2vIv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#  The above model evidences the drastic increase in model performance\n",
        "# due to normalization. We will consider normalization of numeric features\n",
        "# as a necessity. It will become part of our new baseline for future modifications.\n",
        "\n",
        "\n",
        "# Next step is to examine data imputation techniques and whether we can\n",
        "# maintain low loss and mse evaluations but with more 'fair'\n",
        "# imputation techniques."
      ],
      "metadata": {
        "id": "vzrEi9xvwVtb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "####################################################################\n",
        "## FEATURE SELECTION & EXTRACTION\n",
        "The model maintains exceedingly low loss and mse over its last few epochs.\n",
        "The graph shows similar results, this could suggest either a perfect model or overfitting, with the latter being significantly more likely.\n",
        "\n",
        "To combat overitting, we will perform feature selection/extraction. We wll examine the columns which benefited from the most data imputation.\n",
        "the columns which had the most imputed values. These columns are likely where the discrepancy lies.\n",
        "Less redundant data means less opportunity to make decisions based on noise in the data\n",
        "\n",
        "For the sake of clarity, we will examine feature extraction on our so far most 'optimal' model, x.3 - the model with the lowest loss and mse evaluation metrics.\n",
        "We will increment the number of features by 5 in each step, and we will key the optimal number feature as 'E'"
      ],
      "metadata": {
        "id": "wg2DegCyNcCI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# EXAMINE FEATURES WITH 80% MISSING DATA\n",
        "missing_value_threshold = 3763\n",
        "\n",
        "# Identify columns which have missing values more than the threshold\n",
        "columns_with_missing_values = [col for col in Mdf.columns if Mdf[col].isna().sum() >= missing_value_threshold]\n",
        "\n",
        "print(len(columns_with_missing_values))\n"
      ],
      "metadata": {
        "id": "pW2nlYgHvoiG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "There are (obviously) no categorical features that need to be extracted - none benefit from data imputation.\n",
        "\n",
        "Thus, we can just directly drop the relevant features from model 3.3\n",
        "\n",
        "Labels also need not be touched, as they are LI(PPM)"
      ],
      "metadata": {
        "id": "jj8UmjS65LtQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "five = ['TE(PPM)', 'AU(PPM)', 'HG(PPM)', 'TL(PPM)', 'BI(PPM)']\n",
        "\n",
        "ten = ['SE(PPM)', 'AG(PPM)', 'CD(PPM)', 'IN(PPM)', 'SB(PPM)',\n",
        "       'TE(PPM)', 'AU(PPM)', 'HG(PPM)', 'TL(PPM)', 'BI(PPM)']\n",
        "\n",
        "fifteen = ['DRILLING DEPTH (MAX.)','BE(PPM)', 'B(PPM)', 'GE(PPM)', 'AS(PPM)',\n",
        "           'SE(PPM)', 'AG(PPM)', 'CD(PPM)', 'IN(PPM)', 'SB(PPM)',\n",
        "           'TE(PPM)', 'AU(PPM)', 'HG(PPM)', 'TL(PPM)', 'BI(PPM)']"
      ],
      "metadata": {
        "id": "LviDO5787hvD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 5 features\n",
        "M3_35train_features = M3_3train_features.drop(five, axis = 1)\n",
        "M3_35val_features = M3_3val_features.drop(five, axis = 1)"
      ],
      "metadata": {
        "id": "kKEXqMoX6ggk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 10 features\n",
        "M3_310train_features = M3_3train_features.drop(ten, axis = 1)\n",
        "M3_310val_features = M3_3val_features.drop(ten, axis = 1)"
      ],
      "metadata": {
        "id": "BBO_0dC66j4i"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 15 features\n",
        "M3_315train_features = M3_3train_features.drop(fifteen, axis = 1)\n",
        "M3_315val_features = M3_3val_features.drop(fifteen, axis = 1)"
      ],
      "metadata": {
        "id": "5eKbypfM8c8c"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# # MODEL TESTING\n",
        "# model35 = tf.keras.Sequential([\n",
        "#     tf.keras.layers.Dense(128, activation = 'relu', input_shape=(M3_35train_features.shape[1],)),\n",
        "#     tf.keras.layers.Dense(64, activation = 'relu'),\n",
        "#     tf.keras.layers.Dense(32, activation = 'relu'),\n",
        "#     tf.keras.layers.Dense(16, activation = 'relu'),\n",
        "#     tf.keras.layers.Dense(1, activation = 'linear')\n",
        "# ])\n",
        "\n",
        "# model35.compile(loss='mean_squared_error', # MSE because we are predicting a continuous value\n",
        "#                    optimizer = tf.keras.optimizers.Adam(learning_rate=0.001),\n",
        "#                       metrics = ['mse'])\n",
        "\n",
        "# history3_35 = model35.fit(M3_35train_features, M3_3train_labels, epochs=30,\n",
        "#                   batch_size = 32, validation_data = (M3_35val_features, M3_3val_labels))"
      ],
      "metadata": {
        "id": "bLR8jAr_8hKt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# # MODEL TESTING\n",
        "# model310 = tf.keras.Sequential([\n",
        "#     tf.keras.layers.Dense(128, activation = 'relu', input_shape=(M3_310train_features.shape[1],)),\n",
        "#     tf.keras.layers.Dense(64, activation = 'relu'),\n",
        "#     tf.keras.layers.Dense(32, activation = 'relu'),\n",
        "#     tf.keras.layers.Dense(16, activation = 'relu'),\n",
        "#     tf.keras.layers.Dense(1, activation = 'linear')\n",
        "# ])\n",
        "\n",
        "# model310.compile(loss='mean_squared_error', # MSE because we are predicting a continuous value\n",
        "#                    optimizer = tf.keras.optimizers.Adam(learning_rate=0.001),\n",
        "#                       metrics = ['mse'])\n",
        "\n",
        "# history3_310 = model310.fit(M3_310train_features, M3_3train_labels, epochs=30,\n",
        "#                   batch_size = 32, validation_data = (M3_310val_features, M3_3val_labels))"
      ],
      "metadata": {
        "id": "ngYda36u9RCJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# MODEL TESTING\n",
        "model315 = tf.keras.Sequential([\n",
        "    tf.keras.layers.Dense(128, activation = 'relu', input_shape=(M3_315train_features.shape[1],)),\n",
        "    tf.keras.layers.Dense(64, activation = 'relu'),\n",
        "    tf.keras.layers.Dense(32, activation = 'relu'),\n",
        "    tf.keras.layers.Dense(16, activation = 'relu'),\n",
        "    tf.keras.layers.Dense(1, activation = 'linear')\n",
        "])\n",
        "\n",
        "model315.compile(loss='mean_squared_error', # MSE because we are predicting a continuous value\n",
        "                   optimizer = tf.keras.optimizers.Adam(learning_rate=0.001),\n",
        "                      metrics = ['mse'])\n",
        "\n",
        "history3_315 = model315.fit(M3_315train_features, M3_3train_labels, epochs=30,\n",
        "                  batch_size = 32, validation_data = (M3_315val_features, M3_3val_labels))"
      ],
      "metadata": {
        "id": "hCbApDUq9aPi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The models suggest that feature extraction has not rectified the overfitting on the data set.\n",
        "The overfitting may be as a result of solely our imputation technique(s) or a combination of both imputation and noisy features.\n",
        "\n",
        "As next steps, we will examine different imputation techniques, combine imputation with feature selection, and examine the effects of Regularization.\n",
        "We will only consider the two normalized data sets."
      ],
      "metadata": {
        "id": "gectpIHh-XMf"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "NJ5GmkjN-r-P"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}
